{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3645cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import emot \n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import gensim\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from num2words import num2words\n",
    "from nltk.corpus import stopwords\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48638f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseClean:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        short_form_list = open('short_forms.txt', 'r')\n",
    "        chat_words_str = short_form_list.read()\n",
    "        self.chat_words_map_dict = {}\n",
    "        self.chat_words_list = []\n",
    "        for line in chat_words_str.split(\"\\n\"):\n",
    "            if line != \"\":\n",
    "                cw = line.split(\"=\")[0]\n",
    "                try:\n",
    "                    cw_expanded = line.split(\"=\")[1]\n",
    "                except Exception as e:\n",
    "                    print(line)\n",
    "                self.chat_words_list.append(cw)\n",
    "                self.chat_words_map_dict[cw] = cw_expanded\n",
    "        self.chat_words_list = set(self.chat_words_list)\n",
    "        with open(\"contractions.json\",'r') as outfile:\n",
    "            contractions_json_data = outfile.read()\n",
    "        self.contraction_mapping = json.loads(contractions_json_data)\n",
    "        with open(\"emoticons.json\",'r',encoding='utf-8') as outfile:\n",
    "            emoticons_json_data = outfile.read()  \n",
    "        self.EMOTICONS = eval(emoticons_json_data)\n",
    "        with open(\"emoji_word.json\",'r',encoding='utf-8') as outfile:\n",
    "            emoji_word_json_data = outfile.read()  \n",
    "        self.EMO_UNICODE = eval(emoji_word_json_data)\n",
    "        self.UNICODE_EMO = {v: k for k, v in self.EMO_UNICODE.items()}\n",
    "        self.emot_obj = emot.core.emot() \n",
    "        stopwords_nltk = list(stopwords.words('english')) \n",
    "        self.sp = spacy.load('en_core_web_sm')\n",
    "        stopwords_spacy = list(self.sp.Defaults.stop_words)\n",
    "        stopwords_gensim = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "        self.all_stopwords = []\n",
    "        self.all_stopwords.extend(stopwords_nltk)\n",
    "        self.all_stopwords.extend(stopwords_spacy)\n",
    "        self.all_stopwords.extend(stopwords_gensim)\n",
    "        self.all_stopwords = list(set(self.all_stopwords))\n",
    "    \n",
    "    def get_tokens(self,text):\n",
    "        text_tokenized = []\n",
    "        doc = self.nlp(text)\n",
    "        word_tokenized_list = [token.text for token in doc]\n",
    "        return word_tokenized_list\n",
    "        \n",
    "    def convert_to_lower_case(self,text):\n",
    "        text_lower = text.lower()\n",
    "        return text_lower\n",
    "\n",
    "    def remove_html_tags_regex(self,text):\n",
    "        html_pattern = r'<.*?>'\n",
    "        text_without_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
    "        return text_without_html\n",
    "\n",
    "    def remove_html_tags_beautifulsoup(self,text):\n",
    "        parser = BeautifulSoup(text, \"html.parser\")\n",
    "        text_without_html = parser.get_text(separator = \" \")\n",
    "        return text_without_html\n",
    "\n",
    "    def remove_urls_regex(self,text):\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        text_without_urls = re.sub(pattern=url_pattern, repl=' ', string=text)\n",
    "        return text_without_urls\n",
    "\n",
    "    def remove_numbers_regex(self,text):\n",
    "        number_pattern = r'\\d+'\n",
    "        text_without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "        return text_without_number\n",
    "\n",
    "    def convert_numbers_to_words(self,text):\n",
    "        text_tokenized = self.get_tokens(text)\n",
    "        for index in range(len(text_tokenized)):\n",
    "            if text_tokenized[index].isdigit():\n",
    "                text_tokenized[index] = num2words(text_tokenized[index])\n",
    "        numbers_to_words = ' '.join(text_tokenized)\n",
    "        return numbers_to_words\n",
    "\n",
    "\n",
    "    def convert_accented_to_ascii(self,text):\n",
    "        unaccented_text = str(unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
    "        if unaccented_text == None:\n",
    "            return \"\"\n",
    "        else:\n",
    "            return unaccented_text\n",
    "\n",
    "    def convert_chat_abbreviations(self,text):\n",
    "        text_tokenized = self.get_tokens(text)\n",
    "        new_text = []\n",
    "        for w in text_tokenized:\n",
    "            if w.upper() in self.chat_words_list:\n",
    "                new_text.append(self.chat_words_map_dict[w.upper()])\n",
    "            else:\n",
    "                new_text.append(w)\n",
    "        expanded_text = \" \".join(new_text)\n",
    "        return expanded_text\n",
    "\n",
    "    def expand_contractions(self,text):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(self.contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = self.contraction_mapping.get(match)                                    if self.contraction_mapping.get(match)                                    else self.contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            if expanded_contraction == None:\n",
    "                return \"\"\n",
    "            else:\n",
    "                return expanded_contraction\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "    def remove_emoji_regex(self,text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  \n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                                   u\"\\U00002500-\\U00002BEF\"  \n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\"\n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\" \n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        text_without_emoji = emoji_pattern.sub(r'',text)\n",
    "        return text_without_emoji\n",
    "\n",
    "    def remove_emoji_emot(self,text):\n",
    "        for val in self.emot_obj.emoji(text)['value']:\n",
    "            text = text.replace(val,'')\n",
    "        return text\n",
    "\n",
    "    def remove_emoticons_regex(self,text):\n",
    "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in self.EMOTICONS) + u')')\n",
    "        text_without_emoticons = emoticon_pattern.sub(r'',text)\n",
    "        return text_without_emoticons\n",
    "\n",
    "    def remove_emoticons_emot(self,text):\n",
    "        for val in self.emot_obj.emoticons(text)['value']:\n",
    "            text = text.replace(val,'')\n",
    "        return text\n",
    "\n",
    "    def convert_emoji_to_word(self,text):\n",
    "        for emot in self.UNICODE_EMO:\n",
    "            emoji_pattern = r'('+emot+')'\n",
    "            emoji_words = self.UNICODE_EMO[emot]\n",
    "            replace_text = emoji_words.replace(\",\",\"\")\n",
    "            replace_text = replace_text.replace(\":\",\"\")\n",
    "            replace_text_list = replace_text.split()\n",
    "            emoji_name = '_'.join(replace_text_list)\n",
    "            text = re.sub(emoji_pattern, emoji_name, text)\n",
    "        return text\n",
    "\n",
    "    def convert_emoticons_to_word(self,text):\n",
    "        for emot in self.EMOTICONS:\n",
    "            emoticon_pattern = r'('+emot+')'\n",
    "            emoticon_words = self.EMOTICONS[emot]\n",
    "            replace_text = emoticon_words.replace(\",\",\"\")\n",
    "            replace_text = replace_text.replace(\":\",\"\")\n",
    "            replace_text_list = replace_text.split()\n",
    "            emoticon_name = '_'.join(replace_text_list)\n",
    "            text = re.sub(emoticon_pattern, emoticon_name, text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self,text):\n",
    "        text_without_punctuations = text.translate(str.maketrans('', '', punctuation))\n",
    "        return text_without_punctuations\n",
    "\n",
    "    def remove_stopwords(self,text):\n",
    "        text_without_sw = []\n",
    "        text_tokenized = self.get_tokens(text)\n",
    "        for word in text_tokenized:\n",
    "            if word not in self.all_stopwords:\n",
    "                text_without_sw.append(word)\n",
    "        text_without_stopwords = ' '.join(text_without_sw)\n",
    "        return text_without_stopwords\n",
    "\n",
    "    def remove_extra_whitespaces(self,text):\n",
    "        space_pattern = r'\\s+'\n",
    "        text_without_space = re.sub(pattern=space_pattern, repl=\" \", string=text)\n",
    "        return text_without_space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "eab4bd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from attention import Attention\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b5a1843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    def __init__(self,dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.noise_clean_ob = NoiseClean()\n",
    "            \n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.dataset_path)\n",
    "        \n",
    "    def clean_text(self,text):\n",
    "        text = self.noise_clean_ob.convert_accented_to_ascii(text)\n",
    "        text = self.noise_clean_ob.remove_urls_regex(text)\n",
    "        text = self.noise_clean_ob.remove_html_tags_regex(text)\n",
    "        text = self.noise_clean_ob.remove_html_tags_beautifulsoup(text)\n",
    "        text = self.noise_clean_ob.convert_chat_abbreviations(text)\n",
    "        text = self.noise_clean_ob.expand_contractions(text)\n",
    "        text = self.noise_clean_ob.remove_emoji_emot(text)\n",
    "        text = self.noise_clean_ob.remove_emoji_regex(text)\n",
    "        text = self.noise_clean_ob.remove_emoticons_emot(text)\n",
    "        text = self.noise_clean_ob.remove_emoticons_regex(text)\n",
    "        #text = self.noise_clean_ob.convert_numbers_to_words(text)\n",
    "        #text = self.noise_clean_ob.remove_punctuations(text)\n",
    "        text = self.noise_clean_ob.remove_extra_whitespaces(text)\n",
    "        return text\n",
    "    \n",
    "    def clean_data(self,column_name):\n",
    "        print(\"\\nCleaning Data ---->\")\n",
    "        self.df['cleaned_tweet_text'] = self.df.progress_apply(lambda row:self.clean_text(row[column_name]),axis=1)\n",
    "        \n",
    "    def get_class_distribution(self):\n",
    "        print(\"\\nClass Distribution of Dataset ----> {0}\".format(self.dataset_path))\n",
    "        self.class_distribution_df = self.df.groupby(['label_text']).count().drop(columns=self.df.columns.tolist()[:len(self.df.columns.tolist())-2]).reset_index()\n",
    "        self.class_distribution_df.rename(columns={self.class_distribution_df.columns[1]:'count'},inplace=True)\n",
    "        self.class_distribution_df.plot.bar(x='label_text',y='count',rot=0,figsize=(20,10))\n",
    "\n",
    "    def data_split(self):\n",
    "        print(\"\\nSplitting Data into Train, Test and Validation Sample ----> \")\n",
    "        self.label_encoder_target = LabelEncoder()\n",
    "        self.df['label_text_transformed'] = self.label_encoder_target.fit_transform(self.df['label_text'])\n",
    "        self.label_encoder_mappings_dict = dict(zip(self.label_encoder_target.transform(self.label_encoder_target.classes_),self.label_encoder_target.classes_))\n",
    "        self.trainD, self.testD = train_test_split(self.df, test_size=0.2, random_state=100, stratify=self.df[\"label_text_transformed\"])\n",
    "        self.testD, self.valD = train_test_split(self.testD, test_size=0.5, random_state=100)\n",
    "        print(\"Training Sample Shape ----> \", self.trainD.shape)\n",
    "        print(\"Testing Sample Shape ----> \", self.testD.shape)\n",
    "        print(\"Validation Sample Shape ----> \", self.valD.shape)\n",
    "        self.X_train = self.trainD[\"cleaned_tweet_text\"].tolist() \n",
    "        self.y_train = self.trainD[\"label_text_transformed\"].tolist()\n",
    "        self.X_test = self.testD[\"cleaned_tweet_text\"].tolist() \n",
    "        self.y_test = self.testD[\"label_text_transformed\"].tolist()\n",
    "        self.X_val = self.valD[\"cleaned_tweet_text\"].tolist() \n",
    "        self.y_val = self.valD[\"label_text_transformed\"].tolist()\n",
    "\n",
    "    def convert_data_into_tensorflow_format(self):\n",
    "        print(\"\\nConverting Data into Tensorflow Format ----> \")\n",
    "        BUFFER_SIZE = 8000\n",
    "        BATCH_SIZE = 32\n",
    "        self.train_dataset = tf.data.Dataset.from_tensor_slices((self.X_train, self.y_train))\n",
    "        self.test_dataset = tf.data.Dataset.from_tensor_slices((self.X_test, self.y_test))\n",
    "        self.val_dataset = tf.data.Dataset.from_tensor_slices((self.X_val, self.y_val))\n",
    "        self.train_dataset = self.train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        self.test_dataset = self.test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        self.val_dataset = self.val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    def vectorize_tweets(self,ngram):\n",
    "        print(\"\\nVectorizing Tweet using {0},{1} ngram ----> \".format(ngram[0],ngram[1]))\n",
    "        self.vectorizer_model = TfidfVectorizer(ngram_range=ngram)\n",
    "        self.vectorizer_model.fit(self.df['cleaned_tweet_text'].tolist())\n",
    "        \"\"\"print(\"\\nVocabulary Length ----> \",len(self.vectorizer_model.vocabulary_))\"\"\"\n",
    "        self.X_train_idf = self.vectorizer_model.transform(self.X_train)\n",
    "        self.X_test_idf = self.vectorizer_model.transform(self.X_test)\n",
    "    \n",
    "    def get_precision_recall_f1score(self,y_true,y_predicted,average_across_all_classes=True):\n",
    "        prf1_df = pd.DataFrame(precision_recall_fscore_support(y_true, y_predicted),columns=self.label_encoder_target.transform(self.label_encoder_target.classes_))\n",
    "        prf1_df = prf1_df.iloc[:-1]\n",
    "        if average_across_all_classes:\n",
    "            self.average = prf1_df.mean(axis=1)\n",
    "        else:\n",
    "            self.average = prf1_df.values.tolist()\n",
    "        \"\"\"return average\"\"\"\n",
    "    \n",
    "    def train_naive_bayes(self):\n",
    "        print(\"\\nTraining Multinomial Naive Bayes Algorithm ----> \")\n",
    "        mnb_model = naive_bayes.MultinomialNB()\n",
    "        mnb_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = mnb_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nNaive Bayes Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_support_vector_machines(self):\n",
    "        print(\"\\nTraining Support Vector Machine Algorithm ----> \")\n",
    "        svm_model = svm.SVC(random_state=29)\n",
    "        svm_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = svm_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nSVM Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_random_forest(self):\n",
    "        print(\"\\nTraining Random Forest Algorithm ----> \")\n",
    "        rf_model = RandomForestClassifier(random_state=29)\n",
    "        rf_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = rf_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nRandom Forest Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_decision_tree(self):\n",
    "        print(\"\\nTraining Decision Tree Algorithm ----> \")\n",
    "        dt_model = DecisionTreeClassifier(random_state=29)\n",
    "        dt_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = dt_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nDecision Tree Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_logistic_regression(self):\n",
    "        print(\"\\nTraining Logistic Regression Algorithm ----> \")\n",
    "        lr_model = LogisticRegression(random_state=29)\n",
    "        lr_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = lr_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nLogistic Regression Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_knn(self):\n",
    "        print(\"\\nTraining K Nearest Neighbor Algorithm ----> \")\n",
    "        knn_model = KNeighborsClassifier()\n",
    "        knn_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = knn_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nK Nearest Neighbor Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_gradient_boosting(self):\n",
    "        print(\"\\nTraining Gradient Boosting Algorithm ----> \")\n",
    "        gb_model = GradientBoostingClassifier(random_state=29)\n",
    "        gb_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = gb_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nGradient Boosting Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_xgboost(self):\n",
    "        print(\"\\nTraining XGBoost Algorithm ----> \")\n",
    "        xg_train = xgb.DMatrix(cf.X_train_idf, label=cf.y_train)\n",
    "        xg_test = xgb.DMatrix(cf.X_test_idf, label=cf.y_test)\n",
    "        param = {}\n",
    "        param['objective'] = 'multi:softmax'\n",
    "        param['eta'] = 0.1\n",
    "        param['max_depth'] = 6\n",
    "        param['nthread'] = 4\n",
    "        param['num_class'] = 6\n",
    "        watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "        num_round = 5\n",
    "        xgb_model = xgb.train(param, xg_train, num_round, watchlist)\n",
    "        predicted_val = xgb_model.predict(xg_test)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nXGBoost Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    def train_catboost_classifier(self):\n",
    "        print(\"\\nTraining CatBoost Algorithm ----> \")\n",
    "        cb_model = CatBoostClassifier(random_state=29,iterations=100,learning_rate=0.01,eval_metric='Accuracy',task_type='CPU')\n",
    "        cb_model.fit(self.X_train_idf,self.y_train)\n",
    "        predicted_val = cb_model.predict(self.X_test_idf)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nCatBoost Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "\n",
    "    \n",
    "    def plot_graphs(self,history, metric):\n",
    "        plt.plot(history.history[metric])\n",
    "        plt.plot(history.history['val_'+metric], '')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend([metric, 'val_'+metric])\n",
    "    \n",
    "    def train_lstm_without_embedding(self):\n",
    "        print(\"\\nTraining LSTM without Embeddings ----> \")\n",
    "        VOCAB_SIZE = 10000\n",
    "        encoder_model = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "        encoder_model.adapt(self.train_dataset.map(lambda text, label: text))\n",
    "        lstm_model = tf.keras.Sequential([encoder_model,\n",
    "            tf.keras.layers.Embedding(input_dim=len(encoder_model.get_vocabulary()),output_dim=128,mask_zero=True),\n",
    "            tf.keras.layers.LSTM(64, dropout=0.2),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(1)])\n",
    "        lstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "        history = lstm_model.fit(self.train_dataset, epochs=5,validation_data=self.val_dataset)\n",
    "        test_loss, test_acc = lstm_model.evaluate(self.test_dataset)\n",
    "        predictions = lstm_model.predict(self.test_dataset)\n",
    "        predicted_val = np.argmax(predictions, axis=-1)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nLSTM without Embeddings Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "        \"\"\"print('LSTM WITHOUT EMBEDDING TEST LOSS:', test_loss)\n",
    "        print('LSTM WITHOUT EMBEDDING TEST ACCURACY:', test_acc)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        self.plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\"\"\"\n",
    "    \n",
    "    def train_lstm_with_glove(self):\n",
    "        print(\"\\nTraining LSTM with Glove ----> \")\n",
    "        vectorizer_model = TextVectorization(max_tokens=14000, output_sequence_length=200)\n",
    "        vectorizer_model.adapt(self.train_dataset.map(lambda text, label: text))\n",
    "        vocab = vectorizer_model.get_vocabulary()\n",
    "        word_index_dict = dict(zip(vocab, range(len(vocab))))\n",
    "        glove_file_path = \"glove.6B.100d.txt\"\n",
    "        embeddings_index_dict = {}\n",
    "        with open(glove_file_path,errors='ignore') as file:\n",
    "            for data in file:\n",
    "                word, coefs = data.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                embeddings_index_dict[word] = coefs\n",
    "        print(\"Found {0} Word Embeddings\".format(len(embeddings_index_dict)))\n",
    "        num_tokens = 14000 + 2\n",
    "        embedding_dimension = 100\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        embedding_matrix = np.zeros((num_tokens, embedding_dimension))\n",
    "        for word, i in word_index_dict.items():\n",
    "            embedding_vector = embeddings_index_dict.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            else:\n",
    "                misses += 1\n",
    "        print(\"Converted {0} Words ({1} Misses)\".format(hits, misses))\n",
    "        embedding_layer = Embedding(num_tokens,\n",
    "                                    embedding_dimension,\n",
    "                                    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                                    trainable=False,\n",
    "                                    name=\"Embeddings\")\n",
    "        lstm_glove_model = tf.keras.Sequential()\n",
    "        lstm_glove_model.add(vectorizer_model)\n",
    "        lstm_glove_model.add(embedding_layer)\n",
    "        lstm_glove_model.add(tf.keras.layers.LSTM(128, dropout=0.2, return_sequences=True))\n",
    "        lstm_glove_model.add(Attention())\n",
    "        lstm_glove_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        lstm_glove_model.add(tf.keras.layers.Dropout(0.2))\n",
    "        lstm_glove_model.add(tf.keras.layers.Dense(1))\n",
    "        lstm_glove_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "        tf.keras.utils.plot_model(lstm_glove_model)\n",
    "        history = lstm_glove_model.fit(self.train_dataset, epochs=9,validation_data=self.val_dataset)\n",
    "        test_loss, test_acc = lstm_glove_model.evaluate(self.test_dataset)\n",
    "        predictions = lstm_glove_model.predict(self.test_dataset)\n",
    "        predicted_val = np.argmax(predictions, axis=-1)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nLSTM with GLOVE Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "        \"\"\"print('LSTM WITH GLOVE TEST LOSS:', test_loss)\n",
    "        print('LSTM WITH GLOVE TEST ACCURACY:', test_acc)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        self.plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\"\"\"\n",
    "\n",
    "    def train_lstm_with_attention(self):\n",
    "        print(\"\\nTraining LSTM with Attention ----> \")\n",
    "        VOCAB_SIZE = 10000\n",
    "        encoder_model = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "        encoder_model.adapt(self.train_dataset.map(lambda text, label: text))\n",
    "        lstm_attention_model = tf.keras.Sequential()\n",
    "        lstm_attention_model.add(encoder_model)\n",
    "        lstm_attention_model.add(tf.keras.layers.Embedding(\n",
    "                input_dim=len(encoder_model.get_vocabulary()),\n",
    "                output_dim=128,\n",
    "                mask_zero=True))\n",
    "        lstm_attention_model.add(tf.keras.layers.LSTM(64, dropout=0.2, return_sequences=True))\n",
    "        lstm_attention_model.add(Attention())\n",
    "        lstm_attention_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        lstm_attention_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        lstm_attention_model.add(tf.keras.layers.Dense(1))\n",
    "        lstm_attention_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "        history = lstm_attention_model.fit(self.train_dataset, epochs=5,validation_data=self.val_dataset)\n",
    "        test_loss, test_acc = lstm_attention_model.evaluate(self.test_dataset)\n",
    "        predictions = lstm_attention_model.predict(self.test_dataset)\n",
    "        predicted_val = np.argmax(predictions, axis=-1)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nLSTM with ATTENTION Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "        \"\"\"print('LSTM WITH ATTENTION TEST LOSS:', test_loss)\n",
    "        print('LSTM WITH ATTENTION TEST ACCURACY:', test_acc)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        self.plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\"\"\"\n",
    "        \n",
    "    def train_bilstm_without_embedding(self):\n",
    "        print(\"\\nTraining Bidirectional LSTM without Embeddings ----> \")\n",
    "        VOCAB_SIZE = 10000\n",
    "        encoder_model = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "        encoder_model.adapt(self.train_dataset.map(lambda text, label: text))\n",
    "        bilstm_model = tf.keras.Sequential([encoder_model,\n",
    "                       tf.keras.layers.Embedding(input_dim=len(encoder_model.get_vocabulary()),output_dim=128,mask_zero=True),\n",
    "                       tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2)),\n",
    "                       tf.keras.layers.Dense(256, activation='relu'),\n",
    "                       tf.keras.layers.Dropout(0.5),\n",
    "                       tf.keras.layers.Dense(1)])\n",
    "        bilstm_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "        history = bilstm_model.fit(self.train_dataset, epochs=5,validation_data=self.val_dataset)\n",
    "        test_loss, test_acc = bilstm_model.evaluate(self.test_dataset)\n",
    "        predictions = bilstm_model.predict(self.test_dataset)\n",
    "        predicted_val = np.argmax(predictions, axis=-1)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nBIDIRECTIONAL LSTM without EMBEDDINGS Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "        \"\"\"print('BIDIRECTIONAL LSTM WITHOUT EMBEDDING TEST LOSS:', test_loss)\n",
    "        print('BIDIRECTIONAL LSTM WITHOUT EMBEDDING TEST ACCURACY:', test_acc)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        self.plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\"\"\" \n",
    "        \n",
    "    def train_bilstm_with_glove(self):\n",
    "        print(\"\\nTraining BIDIRECTIONAL LSTM with Glove ----> \")\n",
    "        vectorizer_model = TextVectorization(max_tokens=14000, output_sequence_length=200)\n",
    "        vectorizer_model.adapt(self.train_dataset.map(lambda text, label: text))\n",
    "        vocab = vectorizer_model.get_vocabulary()\n",
    "        word_index_dict = dict(zip(vocab, range(len(vocab))))\n",
    "        glove_file_path = \"glove.6B.100d.txt\"\n",
    "        embeddings_index_dict = {}\n",
    "        with open(glove_file_path,errors='ignore') as file:\n",
    "            for data in file:\n",
    "                word, coefs = data.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                embeddings_index_dict[word] = coefs\n",
    "        print(\"Found {0} Word Embeddings\".format(len(embeddings_index_dict)))\n",
    "        num_tokens = 14000 + 2\n",
    "        embedding_dimension = 100\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        embedding_matrix = np.zeros((num_tokens, embedding_dimension))\n",
    "        for word, i in word_index_dict.items():\n",
    "            embedding_vector = embeddings_index_dict.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            else:\n",
    "                misses += 1\n",
    "        print(\"Converted {0} Words ({1} Misses)\".format(hits, misses))\n",
    "        embedding_layer = Embedding(num_tokens,\n",
    "                                    embedding_dimension,\n",
    "                                    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                                    trainable=False,\n",
    "                                    name=\"Embeddings\")\n",
    "        lstm_glove_model = tf.keras.Sequential()\n",
    "        lstm_glove_model.add(vectorizer_model)\n",
    "        lstm_glove_model.add(embedding_layer)\n",
    "        lstm_glove_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, return_sequences=True)))\n",
    "        lstm_glove_model.add(Attention())\n",
    "        lstm_glove_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        lstm_glove_model.add(tf.keras.layers.Dropout(0.2))\n",
    "        lstm_glove_model.add(tf.keras.layers.Dense(1))\n",
    "        lstm_glove_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "        tf.keras.utils.plot_model(lstm_glove_model)\n",
    "        history = lstm_glove_model.fit(self.train_dataset, epochs=9,validation_data=self.val_dataset)\n",
    "        test_loss, test_acc = lstm_glove_model.evaluate(self.test_dataset)\n",
    "        predictions = lstm_glove_model.predict(self.test_dataset)\n",
    "        predicted_val = np.argmax(predictions, axis=-1)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nBIDIRECTIONAL LSTM with GLOVE Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "        \"\"\"print('BIDIRECTIONAL LSTM WITH GLOVE TEST LOSS:', test_loss)\n",
    "        print('BIDIRECTIONAL LSTM WITH GLOVE TEST ACCURACY:', test_acc)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        self.plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\"\"\"\n",
    "\n",
    "    def train_bilstm_with_attention(self):\n",
    "        print(\"\\nTraining BIDIRECTIONAL LSTM with Attention ----> \")\n",
    "        VOCAB_SIZE = 10000\n",
    "        encoder_model = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "        encoder_model.adapt(self.train_dataset.map(lambda text, label: text))\n",
    "        lstm_attention_model = tf.keras.Sequential()\n",
    "        lstm_attention_model.add(encoder_model)\n",
    "        lstm_attention_model.add(tf.keras.layers.Embedding(\n",
    "                input_dim=len(encoder_model.get_vocabulary()),\n",
    "                output_dim=128,\n",
    "                mask_zero=True))\n",
    "        lstm_attention_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2, return_sequences=True)))\n",
    "        lstm_attention_model.add(Attention())\n",
    "        lstm_attention_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        lstm_attention_model.add(tf.keras.layers.Dropout(0.5))\n",
    "        lstm_attention_model.add(tf.keras.layers.Dense(1))\n",
    "        lstm_attention_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                      metrics=['accuracy'])\n",
    "        history = lstm_attention_model.fit(self.train_dataset, epochs=5,validation_data=self.val_dataset)\n",
    "        test_loss, test_acc = lstm_attention_model.evaluate(self.test_dataset)\n",
    "        predictions = lstm_attention_model.predict(self.test_dataset)\n",
    "        predicted_val = np.argmax(predictions, axis=-1)\n",
    "        self.get_precision_recall_f1score(self.y_test,predicted_val)\n",
    "        print(\"\\nBIDIRECTIONAL LSTM with ATTENTION Accuracy ----> \",accuracy_score(predicted_val, self.y_test)*100)\n",
    "        \"\"\"print('LSTM WITH ATTENTION TEST LOSS:', test_loss)\n",
    "        print('LSTM WITH ATTENTION TEST ACCURACY:', test_acc)\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        self.plot_graphs(history, 'accuracy')\n",
    "        plt.ylim(None, 1)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        self.plot_graphs(history, 'loss')\n",
    "        plt.ylim(0, None)\"\"\"   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "781301ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_path in [\"hurricane.csv\",\"flood.csv\",\"earthquake.csv\",\"wildfire.csv\"]:\n",
    "    cf = Classification(dataset_path)\n",
    "    cf.load_data()\n",
    "    cf.get_class_distribution()\n",
    "    cf.clean_data(column_name='tweet_text')\n",
    "    cf.data_split()\n",
    "    cf.convert_data_into_tensorflow_format()\n",
    "    classical_ml_report_list = []\n",
    "    for model in ['KNN','Logistic Regression','Naive Bayes','Decision Tree','Random Forest','SVM','Gradient Boosting','XGBoost','CatBoost']:\n",
    "        ngrams = [(1,1),(2,2),(3,3),(1,2),(2,3),(1,3)]\n",
    "        #ngrams = [(1,1),(2,2),(3,3),(2,3),(1,2,3)]\n",
    "        for gram in ngrams:\n",
    "            tmp_report_list = []\n",
    "            tmp_report_list.append(model)\n",
    "            if gram == (1,3):\n",
    "                tmp_report_list.append((1,2,3))\n",
    "            else:\n",
    "                tmp_report_list.append(gram)\n",
    "            cf.vectorize_tweets(gram)\n",
    "            if model == 'KNN':\n",
    "                cf.train_knn()\n",
    "            elif model == 'Logistic Regression':\n",
    "                cf.train_logistic_regression()\n",
    "            elif model == 'Naive Bayes':\n",
    "                cf.train_naive_bayes()\n",
    "            elif model == 'Decision Tree':\n",
    "                cf.train_decision_tree()\n",
    "            elif model == 'Random Forest':\n",
    "                cf.train_random_forest()\n",
    "            elif model == 'SVM':\n",
    "                cf.train_support_vector_machines()\n",
    "            elif model == 'Gradient Boosting':\n",
    "                cf.train_gradient_boosting()\n",
    "            elif model == 'XGBoost':\n",
    "                cf.train_xgboost()\n",
    "            elif model == 'CatBoost':\n",
    "                cf.train_catboost_classifier()\n",
    "            tmp_report_list.extend(cf.average)\n",
    "            classical_ml_report_list.append(tmp_report_list)\n",
    "    classical_ml_report_df = pd.DataFrame(classical_ml_report_list,columns=['Classifier','Ngram','Precision','Recall','F1 Score'])\n",
    "    classical_ml_report_df.to_csv('classical_ml_report_'+dataset_path,index=False)\n",
    "    deep_nn_report_list = []\n",
    "    for model in ['LSTM','Bi-LSTM']:\n",
    "        for category in ['Without Embedding','With Embedding','With Attention']:\n",
    "            tmp_report_list = []\n",
    "            tmp_report_list.append(model)\n",
    "            tmp_report_list.append(category)\n",
    "            if model == 'LSTM':\n",
    "                if category == 'Without Embedding':\n",
    "                    cf.train_lstm_without_embedding()\n",
    "                elif category == 'With Embedding':\n",
    "                    cf.train_lstm_with_glove()\n",
    "                elif category == 'With Attention':\n",
    "                    cf.train_lstm_with_attention()\n",
    "            elif model == 'Bi-LSTM':\n",
    "                if category == 'Without Embedding':\n",
    "                    cf.train_bilstm_without_embedding()\n",
    "                elif category == 'With Embedding':\n",
    "                    cf.train_bilstm_with_glove()\n",
    "                elif category == 'With Attention':\n",
    "                    cf.train_bilstm_with_attention()\n",
    "            tmp_report_list.extend(cf.average)\n",
    "            deep_nn_report_list.append(tmp_report_list)   \n",
    "    deep_nn_report_df = pd.DataFrame(deep_nn_report_list,columns=['Classifier','Embedding','Precision','Recall','F1 Score'])\n",
    "    deep_nn_report_df.to_csv('deep_nn_report_'+dataset_path,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0674782e80fe76cdafa01c42e1ecf76036ae81de091792b76700c7fc0e3fe17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
